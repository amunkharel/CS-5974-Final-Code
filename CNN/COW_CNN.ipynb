{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Environment Setup**\n",
        "\n",
        "1. Mount our google drive directory\n",
        "2. Set the current directory in our python runtime\n",
        "\n",
        "IF NOT RUNNING IN GOOGLE COLAB, CHANGE THIS!"
      ],
      "metadata": {
        "id": "f2VbmQcXxnob"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmuEtxRnxiJ1",
        "outputId": "ab9a47c7-99b9-450f-fac8-b32c4c2b7885"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/School/CS5824/FINALPROJECT\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os\n",
        "# os.chdir('/content/drive/MyDrive/Ye/Projects/DairyBCS/BodyWeight') \n",
        "# os.chdir('/content/drive/MyDrive/Students/Ye/Projects/DairyBCS/BodyWeight')\n",
        "os.chdir('/content/drive/MyDrive/School/CS5824/FINALPROJECT/')\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Necessary Imports**"
      ],
      "metadata": {
        "id": "buzqPfl4yZdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##opencv\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "from glob import glob #read img path.\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import os.path\n",
        "\n",
        "##tensorflow and keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import r2_score"
      ],
      "metadata": {
        "id": "Pf0hDDwtyeEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Parse Drive Directory for Cow Data**\n",
        "\n",
        "REQUIREMENTS:\n",
        "1. The data_dir variable contains the directory which contains each of the day folders.\n",
        "2. Each day folder contains depth, CSV, and RGB subfolders\n",
        "\n",
        "data_dir\n",
        "> D1\n",
        ">> depth\\\n",
        ">> CSV\\\n",
        ">> RGB\n",
        "\n",
        "Where each of the depth, CSV, and RGB folders store folders which have name (COWID)AM or (COWID)PM\n",
        "and store their respective data.\n",
        "\n",
        "The day folders should be formatted like so:\n",
        "\n",
        "D1\n",
        ">depth\n",
        ">>2343AM\n",
        ">>>__123412.png\n",
        "\n",
        ">>2343PM\n",
        ">>>__123412.png"
      ],
      "metadata": {
        "id": "STwGlAY-zCd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Store paths to all cow images\n",
        "import string\n",
        "import re\n",
        "day_regex = 'D\\d+$'\n",
        "data_dir = '/COW_DATA/'\n",
        "root_dir = os.getcwd() + data_dir   # CHANGE THIS TO MATCH YOUR FOLDER STRUCTURE\n",
        "                                    # SHOULD BE WERE ALL THE DX FOLDERS ARE LOCATED\n",
        "day_dirs = list(filter(lambda day: re.search(day_regex, day), os.listdir(root_dir)))\n",
        "print('Inside of: ', root_dir)\n",
        "print('Got information for days: ', ', '.join(day_dirs))\n",
        "depth_dir = '/depth/'\n",
        "csv_dir = '/CSV/'\n",
        "rgb_dir = '/RGB/'\n",
        "\n",
        "cow_depth_paths = np.array([], dtype=np.unicode_)   # Path to all cow depth data folders\n",
        "cow_csv_paths = np.array([], dtype=np.unicode_)     # Path to all cow depth data folders\n",
        "cow_rgb_paths = np.array([], dtype=np.unicode_)     # Path to all cow depth data folders\n",
        "\n",
        "for day in day_dirs:\n",
        "  print('Beginning parse of ', day)\n",
        "  temp_depth_dir = root_dir + day + depth_dir\n",
        "  temp_csv_dir = root_dir + day + csv_dir\n",
        "  temp_rgb_dir = root_dir + day + rgb_dir\n",
        "\n",
        "  try:\n",
        "    print('Attempting to parse depth in ', day)\n",
        "    for cow_dir in os.listdir(temp_depth_dir): # PARSE depth SUBDIRECTORY (/DX/depth/)\n",
        "      cowid = temp_depth_dir + cow_dir\n",
        "      for image in os.listdir(cowid):\n",
        "        path_from_cwd = '.' + data_dir + day + depth_dir + cow_dir + '/'\n",
        "        cow_depth_paths = np.append(cow_depth_paths, path_from_cwd + image)\n",
        "  except FileNotFoundError:\n",
        "    print(day, '/depth was not found. Skipping to the next.')\n",
        "\n",
        "  try:\n",
        "    print('Attempting to parse RGB in ', day)\n",
        "    for cow_dir in os.listdir(temp_rgb_dir): # PARSE RGB SUBDIRECTORY (/DX/CSV/)\n",
        "      cowid = temp_rgb_dir + cow_dir\n",
        "      for image in os.listdir(cowid):\n",
        "        path_from_cwd = '.' + data_dir + day + rgb_dir + cow_dir + '/'\n",
        "        cow_rgb_paths = np.append(cow_rgb_paths, path_from_cwd + image)\n",
        "  except FileNotFoundError:\n",
        "    print(day, '/RGB was not found. Skipping to the next.')\n",
        "\n",
        "  try:\n",
        "    print('Attempting to parse CSV in ', day)\n",
        "    for cow_dir in os.listdir(temp_csv_dir): # PARSE CSV SUBDIRECTORY (/DX/RGB/)\n",
        "      cowid = temp_csv_dir + cow_dir\n",
        "      for image in os.listdir(cowid):\n",
        "        path_from_cwd = '.' + data_dir + day + csv_dir + cow_dir + '/'\n",
        "        cow_csv_paths = np.append(cow_csv_paths, path_from_cwd + image)\n",
        "  except FileNotFoundError:\n",
        "    print(day, '/CSV was not found. Skipping to the next.')\n",
        "    \n",
        "  print('Finished parsing: ', day)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROfh3c3fzBwJ",
        "outputId": "1ece6bb3-62a5-4fc9-a728-0cbdc9f16324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inside of:  /content/drive/MyDrive/School/CS5824/FINALPROJECT/COW_DATA/\n",
            "Got information for days:  D5, D2, D1, D4, D3\n",
            "Beginning parse of  D5\n",
            "Attempting to parse depth in  D5\n",
            "Attempting to parse RGB in  D5\n",
            "Attempting to parse CSV in  D5\n",
            "Finished parsing:  D5\n",
            "Beginning parse of  D2\n",
            "Attempting to parse depth in  D2\n",
            "Attempting to parse RGB in  D2\n",
            "Attempting to parse CSV in  D2\n",
            "Finished parsing:  D2\n",
            "Beginning parse of  D1\n",
            "Attempting to parse depth in  D1\n",
            "Attempting to parse RGB in  D1\n",
            "D1 /RGB was not found. Skipping to the next.\n",
            "Attempting to parse CSV in  D1\n",
            "D1 /CSV was not found. Skipping to the next.\n",
            "Finished parsing:  D1\n",
            "Beginning parse of  D4\n",
            "Attempting to parse depth in  D4\n",
            "Attempting to parse RGB in  D4\n",
            "Attempting to parse CSV in  D4\n",
            "Finished parsing:  D4\n",
            "Beginning parse of  D3\n",
            "Attempting to parse depth in  D3\n",
            "Attempting to parse RGB in  D3\n",
            "Attempting to parse CSV in  D3\n",
            "D3 /CSV was not found. Skipping to the next.\n",
            "Finished parsing:  D3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cow Path Information**\n",
        "\n",
        "Now the paths to all found files will be stored in cow_depth_paths, cow_rgb_paths, and cow_csv_paths respectively.\n",
        "\n",
        "All indices in each of the lists are of the form:\n",
        "\n",
        "./COW_DATA/D5/depth/5687PM/_Depth_61321.png\n",
        "\n",
        "(NOTE THAT THIS PATH IS UNIQUE TO MY OWN ENVIRONMENT. IT MAY LOOK DIFFERENT FOR YOU.)"
      ],
      "metadata": {
        "id": "odDNPF3C0UeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(cow_depth_paths[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9SdvlC147-r",
        "outputId": "ee67d1cf-d6ae-4df9-9993-7fbe6f65acb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./COW_DATA/D5/depth/4973PM/_Depth_60899.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Cow Weight Information Fetching**\n",
        "\n",
        "Now grab the read weights from the csv file.\\\n",
        "The CSV file should be able to be found on the top level of the data_dir directory, on the same\n",
        "level as the D1, D2, ...., DN folders.\n",
        "\n",
        "First, read the CSV file into a dataframe.\\\n",
        "Then we will match up the provided weights from the CSV with the depths they correspond to on a daily basis."
      ],
      "metadata": {
        "id": "E02LVhTi6hDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bw_csv_path = os.getcwd() + data_dir + 'BodyWeight_cleaned.csv'\n",
        "bw_df = pd.read_csv(bw_csv_path) # Read weight csv into a pandas dataframe\n",
        "\n",
        "# Get the largest 'day' number out of all found days\n",
        "largest_day = 0\n",
        "for day in day_dirs:\n",
        "  if (int(day[1:]) > largest_day):\n",
        "    largest_day = int(day[1:])\n",
        "daily_labelled_depth_images = [[] for _ in range(largest_day)]\n",
        "\n",
        "for i in range(len(cow_depth_paths)):\n",
        "  split_depth_path = cow_depth_paths[i].split('/')\n",
        "  day_str = split_depth_path[-4][1:]      # Used to access the weight dataframe\n",
        "  day_idx = int(day_str) - 1              # Take a day string (ie D5), isolate '5' and subtract 1\n",
        "                                          # convert to array index\n",
        "  id = split_depth_path[-2]               # The cowid, of the form XXXXAM or XXXXPM\n",
        "\n",
        "  try:\n",
        "    weight = bw_df[bw_df.DAY == day_str][id].values[0] # grab the weight for given cow\n",
        "  except KeyError:\n",
        "    continue\n",
        "  daily_labelled_depth_images[day_idx].append([cow_depth_paths[i], weight])\n",
        "\n",
        "daily_labelled_depth_images = np.array(daily_labelled_depth_images, dtype=object)"
      ],
      "metadata": {
        "id": "QHCfBm2P7TDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Format Of daily_labelled_depth_images**\n",
        "\n",
        "1st Dimension\\\n",
        "The array daily_labelled_depth_images will contain an array for each day between the largest day value (D10 for example) and the smallest day value (D1). Any folders which happened to not be found or exist in this range will appear as empty lists.\n",
        "\n",
        "2nd Dimension\\\n",
        "The second dimension of daily_labelled_depth_images contains lists of length two which contain\\\n",
        "[PATH_TO_DEPTH_IMAGE, ASSOCIATED WEIGHT]"
      ],
      "metadata": {
        "id": "b2zCQB96Fx1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also create a version of this information where each day's information is combined into a single list, removing the first dimension of daily_labelled_depth_images.\n",
        "\n",
        "This is then converted into a DataFrame with the first column labelled 'FilePath' and the second column labelled 'Weights'"
      ],
      "metadata": {
        "id": "P9qNoYPQGxcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labelled_depth_images = []\n",
        "for day in daily_labelled_depth_images:\n",
        "  labelled_depth_images += day\n",
        "labelled_depth_images = np.array(labelled_depth_images, dtype=object)\n",
        "\n",
        "col_vals = ['FilePath', 'Weights']\n",
        "labelled_depth_images = pd.DataFrame(data=labelled_depth_images, columns=col_vals)\n",
        "labelled_depth_images['FilePath'] = labelled_depth_images['FilePath'].astype(str)\n",
        "labelled_depth_images['Weights'] = labelled_depth_images['Weights'].astype(float)\n",
        "labelled_depth_images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "GMJFU_kFGlbQ",
        "outputId": "f8543771-481e-4139-85a2-7f61cc1cde3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        FilePath  Weights\n",
              "0     ./COW_DATA/D1/depth/5327PM/_Depth_2079.png  1879.81\n",
              "1     ./COW_DATA/D1/depth/5327PM/_Depth_2111.png  1879.81\n",
              "2     ./COW_DATA/D1/depth/5327PM/_Depth_2089.png  1879.81\n",
              "3     ./COW_DATA/D1/depth/5327PM/_Depth_2172.png  1879.81\n",
              "4     ./COW_DATA/D1/depth/4973AM/_Depth_1362.png  1701.97\n",
              "..                                           ...      ...\n",
              "192  ./COW_DATA/D5/depth/5687PM/_Depth_61321.png  1408.75\n",
              "193  ./COW_DATA/D5/depth/5687PM/_Depth_61322.png  1408.75\n",
              "194  ./COW_DATA/D5/depth/5687PM/_Depth_61329.png  1408.75\n",
              "195  ./COW_DATA/D5/depth/5687PM/_Depth_61276.png  1408.75\n",
              "196  ./COW_DATA/D5/depth/5428PM/_Depth_56217.png  1779.13\n",
              "\n",
              "[197 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4a4e9c13-b07b-4e63-a90b-10a105f6f76a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FilePath</th>\n",
              "      <th>Weights</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>./COW_DATA/D1/depth/5327PM/_Depth_2079.png</td>\n",
              "      <td>1879.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>./COW_DATA/D1/depth/5327PM/_Depth_2111.png</td>\n",
              "      <td>1879.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>./COW_DATA/D1/depth/5327PM/_Depth_2089.png</td>\n",
              "      <td>1879.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>./COW_DATA/D1/depth/5327PM/_Depth_2172.png</td>\n",
              "      <td>1879.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>./COW_DATA/D1/depth/4973AM/_Depth_1362.png</td>\n",
              "      <td>1701.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192</th>\n",
              "      <td>./COW_DATA/D5/depth/5687PM/_Depth_61321.png</td>\n",
              "      <td>1408.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193</th>\n",
              "      <td>./COW_DATA/D5/depth/5687PM/_Depth_61322.png</td>\n",
              "      <td>1408.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194</th>\n",
              "      <td>./COW_DATA/D5/depth/5687PM/_Depth_61329.png</td>\n",
              "      <td>1408.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>./COW_DATA/D5/depth/5687PM/_Depth_61276.png</td>\n",
              "      <td>1408.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>./COW_DATA/D5/depth/5428PM/_Depth_56217.png</td>\n",
              "      <td>1779.13</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>197 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4a4e9c13-b07b-4e63-a90b-10a105f6f76a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4a4e9c13-b07b-4e63-a90b-10a105f6f76a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4a4e9c13-b07b-4e63-a90b-10a105f6f76a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create Train / Validation / Test Split, Load Images**\n",
        "\n",
        "Prepare the images for use in the CNN now. Create the train_test_split, then create\n",
        "ImageDataGenerator objects to scale the pixel values for each depth image, and subsequently create\n",
        "a validation set out of the training set."
      ],
      "metadata": {
        "id": "kHeBxCvdOiZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = train_test_split(\n",
        "    labelled_depth_images,\n",
        "    train_size=0.7,\n",
        "    shuffle=True,\n",
        "    random_state=1\n",
        ")"
      ],
      "metadata": {
        "id": "87DsG16tMDiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale =1./255,\n",
        "    validation_split = 0.2\n",
        ")\n",
        "test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale = 1./255\n",
        ")"
      ],
      "metadata": {
        "id": "to4qkKl3PAFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = train_generator.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    x_col='FilePath',\n",
        "    y_col='Weights',\n",
        "    target_size=(120,120),\n",
        "    color_mode='rgb',\n",
        "    class_mode='raw',\n",
        "    batch_size=10,\n",
        "    shuffle=True,\n",
        "    seed=42,\n",
        "    subset='training',\n",
        ")\n",
        "\n",
        "val_images = train_generator.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    x_col='FilePath',\n",
        "    y_col='Weights',\n",
        "    target_size=(120,120),\n",
        "    color_mode='rgb',\n",
        "    class_mode='raw',\n",
        "    batch_size=10,\n",
        "    shuffle=True,\n",
        "    seed=42,\n",
        "    subset='validation',\n",
        ")\n",
        "\n",
        "test_images = test_generator.flow_from_dataframe(\n",
        "    dataframe=test_df,\n",
        "    x_col='FilePath',\n",
        "    y_col='Weights',\n",
        "    target_size=(120,120),\n",
        "    color_mode='rgb',\n",
        "    class_mode='raw',\n",
        "    batch_size=10,\n",
        "    shuffle=False,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ls9Nd-T3PCCZ",
        "outputId": "5ddbf8e7-4f18-4c64-e2bf-7befd4a20a2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 110 validated image filenames.\n",
            "Found 27 validated image filenames.\n",
            "Found 60 validated image filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CNN Design**\n",
        "\n",
        "The code below outlines the architecture of the CNN we use.\n",
        "\n",
        "The first Conv2D layer uses a large inital kernal size because the only feature in the input image is the cow itself, which covers a significant amount of area. Being able to absorb this information spread out across fewer filters will allow us to better capture the relationship between the area it is occupying and its resulting weight prediction - in theory.\n",
        "\n",
        "The remaining layers function have no special modifications."
      ],
      "metadata": {
        "id": "4jnr13v3RMnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.Input(shape=(120,120,3))\n",
        "x = tf.keras.layers.Conv2D(filters=8, kernel_size=(30,30), activation='relu')(inputs)\n",
        "x = tf.keras.layers.AveragePooling2D()(x)\n",
        "x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu')(x)\n",
        "x = tf.keras.layers.AveragePooling2D()(x)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
        "outputs = tf.keras.layers.Dense(1, activation='linear')(x)"
      ],
      "metadata": {
        "id": "B9JM37pnRNGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CNN Training**\n",
        "\n",
        "For training we use the Adam Optimizer and MSE loss.\n",
        "\n",
        "We also perform early stopping checks in the event validation error rises. If the early stopping criteria is not hit, then we will perform 100 epochs of training."
      ],
      "metadata": {
        "id": "4jt6_D3MTO9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = tf.keras.Model(inputs = inputs, outputs = outputs)\n",
        "\n",
        "cnn.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mse'\n",
        ")\n",
        "history = cnn.fit(\n",
        "    train_images,\n",
        "    validation_data=val_images,\n",
        "    epochs=100,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=5,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOxyC-goTGlN",
        "outputId": "0f3d6905-d701-4c78-b2b7-6fb1c3cff26a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "11/11 [==============================] - 9s 768ms/step - loss: 3522124.2500 - val_loss: 3504972.5000\n",
            "Epoch 2/100\n",
            "11/11 [==============================] - 9s 784ms/step - loss: 3459418.2500 - val_loss: 3365802.0000\n",
            "Epoch 3/100\n",
            "11/11 [==============================] - 8s 741ms/step - loss: 3198098.5000 - val_loss: 2919939.5000\n",
            "Epoch 4/100\n",
            "11/11 [==============================] - 8s 740ms/step - loss: 2541270.0000 - val_loss: 1955779.3750\n",
            "Epoch 5/100\n",
            "11/11 [==============================] - 8s 739ms/step - loss: 1329536.3750 - val_loss: 612237.8750\n",
            "Epoch 6/100\n",
            "11/11 [==============================] - 8s 740ms/step - loss: 225133.2969 - val_loss: 82171.2031\n",
            "Epoch 7/100\n",
            "11/11 [==============================] - 8s 739ms/step - loss: 136828.2656 - val_loss: 121923.0391\n",
            "Epoch 8/100\n",
            "11/11 [==============================] - 8s 742ms/step - loss: 68657.6953 - val_loss: 68202.5391\n",
            "Epoch 9/100\n",
            "11/11 [==============================] - 9s 832ms/step - loss: 66188.3672 - val_loss: 68898.6953\n",
            "Epoch 10/100\n",
            "11/11 [==============================] - 10s 830ms/step - loss: 59874.9766 - val_loss: 65115.9922\n",
            "Epoch 11/100\n",
            "11/11 [==============================] - 8s 742ms/step - loss: 56674.6367 - val_loss: 63068.2422\n",
            "Epoch 12/100\n",
            "11/11 [==============================] - 8s 738ms/step - loss: 56102.6367 - val_loss: 63196.0195\n",
            "Epoch 13/100\n",
            "11/11 [==============================] - 8s 739ms/step - loss: 56068.8672 - val_loss: 62877.4727\n",
            "Epoch 14/100\n",
            "11/11 [==============================] - 8s 735ms/step - loss: 55590.9062 - val_loss: 62728.4922\n",
            "Epoch 15/100\n",
            "11/11 [==============================] - 8s 736ms/step - loss: 55478.0625 - val_loss: 62774.6602\n",
            "Epoch 16/100\n",
            "11/11 [==============================] - 8s 743ms/step - loss: 55900.3281 - val_loss: 62662.0352\n",
            "Epoch 17/100\n",
            "11/11 [==============================] - 8s 738ms/step - loss: 55546.0000 - val_loss: 62685.6680\n",
            "Epoch 18/100\n",
            "11/11 [==============================] - 8s 744ms/step - loss: 56266.4531 - val_loss: 63134.2695\n",
            "Epoch 19/100\n",
            "11/11 [==============================] - 8s 740ms/step - loss: 56263.4648 - val_loss: 62573.3398\n",
            "Epoch 20/100\n",
            "11/11 [==============================] - 8s 749ms/step - loss: 55233.3555 - val_loss: 62594.2305\n",
            "Epoch 21/100\n",
            "11/11 [==============================] - 8s 743ms/step - loss: 55687.5898 - val_loss: 62609.1016\n",
            "Epoch 22/100\n",
            "11/11 [==============================] - 8s 744ms/step - loss: 56065.6719 - val_loss: 62910.5664\n",
            "Epoch 23/100\n",
            "11/11 [==============================] - 8s 738ms/step - loss: 56734.9688 - val_loss: 62511.3984\n",
            "Epoch 24/100\n",
            "11/11 [==============================] - 8s 738ms/step - loss: 55779.9141 - val_loss: 62389.3906\n",
            "Epoch 25/100\n",
            "11/11 [==============================] - 8s 748ms/step - loss: 55280.6914 - val_loss: 62914.0820\n",
            "Epoch 26/100\n",
            "11/11 [==============================] - 8s 748ms/step - loss: 55455.8047 - val_loss: 62298.0820\n",
            "Epoch 27/100\n",
            "11/11 [==============================] - 8s 743ms/step - loss: 55931.2773 - val_loss: 62471.9336\n",
            "Epoch 28/100\n",
            "11/11 [==============================] - 8s 749ms/step - loss: 55289.6445 - val_loss: 62947.8203\n",
            "Epoch 29/100\n",
            "11/11 [==============================] - 8s 741ms/step - loss: 55067.9766 - val_loss: 62363.1406\n",
            "Epoch 30/100\n",
            "11/11 [==============================] - 8s 736ms/step - loss: 54803.5625 - val_loss: 62054.9453\n",
            "Epoch 31/100\n",
            "11/11 [==============================] - 8s 742ms/step - loss: 55239.9688 - val_loss: 62061.8438\n",
            "Epoch 32/100\n",
            "11/11 [==============================] - 8s 736ms/step - loss: 54898.6602 - val_loss: 62044.5273\n",
            "Epoch 33/100\n",
            "11/11 [==============================] - 8s 745ms/step - loss: 54690.1641 - val_loss: 61966.8672\n",
            "Epoch 34/100\n",
            "11/11 [==============================] - 8s 753ms/step - loss: 54490.0938 - val_loss: 62385.5352\n",
            "Epoch 35/100\n",
            "11/11 [==============================] - 8s 743ms/step - loss: 54869.2031 - val_loss: 61827.2148\n",
            "Epoch 36/100\n",
            "11/11 [==============================] - 8s 745ms/step - loss: 54536.1562 - val_loss: 61826.5195\n",
            "Epoch 37/100\n",
            "11/11 [==============================] - 8s 737ms/step - loss: 54437.0781 - val_loss: 61700.4453\n",
            "Epoch 38/100\n",
            "11/11 [==============================] - 8s 744ms/step - loss: 54681.1289 - val_loss: 61803.1680\n",
            "Epoch 39/100\n",
            "11/11 [==============================] - 8s 742ms/step - loss: 54298.6367 - val_loss: 61654.1016\n",
            "Epoch 40/100\n",
            "11/11 [==============================] - 8s 744ms/step - loss: 54494.9336 - val_loss: 61592.7539\n",
            "Epoch 41/100\n",
            "11/11 [==============================] - 8s 744ms/step - loss: 54574.3398 - val_loss: 61597.3242\n",
            "Epoch 42/100\n",
            "11/11 [==============================] - 8s 743ms/step - loss: 54076.6172 - val_loss: 61611.2305\n",
            "Epoch 43/100\n",
            "11/11 [==============================] - 11s 976ms/step - loss: 55818.3828 - val_loss: 61370.6836\n",
            "Epoch 44/100\n",
            "11/11 [==============================] - 8s 750ms/step - loss: 55015.5391 - val_loss: 62466.2070\n",
            "Epoch 45/100\n",
            "11/11 [==============================] - 8s 735ms/step - loss: 54946.6719 - val_loss: 62345.1836\n",
            "Epoch 46/100\n",
            "11/11 [==============================] - 8s 739ms/step - loss: 54276.6211 - val_loss: 61323.8672\n",
            "Epoch 47/100\n",
            "11/11 [==============================] - 8s 743ms/step - loss: 54775.9141 - val_loss: 61321.1836\n",
            "Epoch 48/100\n",
            "11/11 [==============================] - 8s 744ms/step - loss: 55093.7227 - val_loss: 61418.4648\n",
            "Epoch 49/100\n",
            "11/11 [==============================] - 8s 743ms/step - loss: 55278.2266 - val_loss: 61409.8320\n",
            "Epoch 50/100\n",
            "11/11 [==============================] - 8s 739ms/step - loss: 53076.6680 - val_loss: 61590.5586\n",
            "Epoch 51/100\n",
            "11/11 [==============================] - 8s 740ms/step - loss: 54802.9883 - val_loss: 61340.8242\n",
            "Epoch 52/100\n",
            "11/11 [==============================] - 8s 744ms/step - loss: 54965.1875 - val_loss: 60859.5547\n",
            "Epoch 53/100\n",
            "11/11 [==============================] - 8s 742ms/step - loss: 53047.0547 - val_loss: 61517.2070\n",
            "Epoch 54/100\n",
            "11/11 [==============================] - 8s 742ms/step - loss: 54784.6016 - val_loss: 61127.7734\n",
            "Epoch 55/100\n",
            "11/11 [==============================] - 8s 748ms/step - loss: 53652.2695 - val_loss: 60680.2852\n",
            "Epoch 56/100\n",
            "11/11 [==============================] - 8s 737ms/step - loss: 53288.2812 - val_loss: 60676.6758\n",
            "Epoch 57/100\n",
            "11/11 [==============================] - 8s 741ms/step - loss: 53221.5820 - val_loss: 60541.7227\n",
            "Epoch 58/100\n",
            "11/11 [==============================] - 8s 741ms/step - loss: 54288.6094 - val_loss: 60648.5938\n",
            "Epoch 59/100\n",
            "11/11 [==============================] - 8s 739ms/step - loss: 55648.4688 - val_loss: 60470.7578\n",
            "Epoch 60/100\n",
            "11/11 [==============================] - 8s 739ms/step - loss: 54106.4180 - val_loss: 60339.6016\n",
            "Epoch 61/100\n",
            "11/11 [==============================] - 8s 740ms/step - loss: 54881.0195 - val_loss: 60273.7656\n",
            "Epoch 62/100\n",
            "11/11 [==============================] - 8s 743ms/step - loss: 52793.9961 - val_loss: 60518.0742\n",
            "Epoch 63/100\n",
            "11/11 [==============================] - 8s 741ms/step - loss: 52960.9375 - val_loss: 60263.1484\n",
            "Epoch 64/100\n",
            "11/11 [==============================] - 8s 751ms/step - loss: 53221.5547 - val_loss: 60173.5000\n",
            "Epoch 65/100\n",
            "11/11 [==============================] - 8s 752ms/step - loss: 52932.6953 - val_loss: 60656.6211\n",
            "Epoch 66/100\n",
            "11/11 [==============================] - 8s 741ms/step - loss: 52025.8867 - val_loss: 60541.5195\n",
            "Epoch 67/100\n",
            "11/11 [==============================] - 8s 755ms/step - loss: 54554.3906 - val_loss: 60743.7852\n",
            "Epoch 68/100\n",
            "11/11 [==============================] - 8s 747ms/step - loss: 51749.2109 - val_loss: 60367.8750\n",
            "Epoch 69/100\n",
            "11/11 [==============================] - 8s 758ms/step - loss: 54218.2617 - val_loss: 59727.3477\n",
            "Epoch 70/100\n",
            "11/11 [==============================] - 8s 746ms/step - loss: 52935.7461 - val_loss: 59953.4453\n",
            "Epoch 71/100\n",
            "11/11 [==============================] - 8s 739ms/step - loss: 54084.3789 - val_loss: 60450.1406\n",
            "Epoch 72/100\n",
            "11/11 [==============================] - 8s 744ms/step - loss: 54020.9102 - val_loss: 60125.2422\n",
            "Epoch 73/100\n",
            "11/11 [==============================] - 8s 744ms/step - loss: 51305.1211 - val_loss: 60690.6172\n",
            "Epoch 74/100\n",
            "11/11 [==============================] - 8s 742ms/step - loss: 51503.1758 - val_loss: 59978.0898\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Making Predictions on Test Set**\n",
        "\n",
        "Here we run the test images through the trained CNN, and get predictions on the weights.\n",
        "\n",
        "We also want to perfrom some amount of backend CV. To do so, select a single cow from each day and grab its associated measured weight, and perform a prediction.\n",
        "\n",
        "These selected values will also be scored in the next block, at the same time as the entire testing set."
      ],
      "metadata": {
        "id": "bvWm_0roTW2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "predicted_weights = np.squeeze(cnn.predict(test_images))\n",
        "true_weights = test_images.labels\n",
        "\n",
        "# Additionally, we also want to select a single cow image from each day to report our\n",
        "# final model metrics on.\n",
        "\n",
        "# Each entry in the selected_cows array corresponds to one index in the daily_labelled_depth_images\n",
        "# array where the randomly grabbed entry resides.\n",
        "selected_cows = []\n",
        "selected_true_weights = []\n",
        "for day in daily_labelled_depth_images:\n",
        "  if (len(day) > 0):\n",
        "    rand_idx = random.randint(0, len(day) - 1)\n",
        "    rand_cow = day[rand_idx]\n",
        "    print(rand_cow)\n",
        "    selected_cows.append(rand_cow)\n",
        "    label_idx = np.where(labelled_depth_images['FilePath'] == rand_cow[0])[0][0]\n",
        "    print(label_idx)\n",
        "    selected_true_weights.append(labelled_depth_images['Weights'][label_idx])\n",
        "  else:\n",
        "    selected_cows.append(['no_cow', 0.0])\n",
        "\n",
        "# Create a dataframe from the found filepaths\n",
        "col_vals = ['FilePath', 'Weights']\n",
        "selected_cows = pd.DataFrame(data=selected_cows, columns=col_vals)\n",
        "selected_cows['FilePath'] = selected_cows['FilePath'].astype(str)\n",
        "selected_cows['Weights'] = selected_cows['Weights'].astype(float)\n",
        "# Now grab those images from the path and place them back in place\n",
        "selected_images = test_generator.flow_from_dataframe(\n",
        "    dataframe=selected_cows,\n",
        "    x_col='FilePath',\n",
        "    y_col='Weights',\n",
        "    target_size=(120,120),\n",
        "    color_mode='rgb',\n",
        "    class_mode='raw',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "selected_predicted_weights = np.squeeze(cnn.predict(selected_images))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAbyJRO0Rk-b",
        "outputId": "84849710-c592-4aa7-95ad-a412d9dfbe12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 1s 205ms/step\n",
            "['./COW_DATA/D1/depth/4973AM/_Depth_1359.png', 1701.97]\n",
            "6\n",
            "['./COW_DATA/D4/depth/5488AM/_Depth_12782.png', 2052.5]\n",
            "98\n",
            "['./COW_DATA/D5/depth/5687PM/_Depth_61321.png', 1408.75]\n",
            "192\n",
            "Found 3 validated image filenames.\n",
            "1/1 [==============================] - 0s 98ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/preprocessing/image.py:989: UserWarning: Found 2 invalid image filename(s) in x_col=\"FilePath\". These filename(s) will be ignored.\n",
            "  warnings.warn('Found {} invalid image filename(s) in x_col=\"{}\". '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Evaluation / Metric Determination**\n",
        "\n",
        "To demonstrate the total quality of the ending model we find the RMSE and R^2 scores.\n",
        "\n",
        "The RMSE demonstrates exactly how far from some regression line our data points are. Smaller is better.\n",
        "\n",
        "The R^2 score demonstrates how much variation in our prediction is explained by the input images. Typically, larger means better. In our case this is the degree to which our predictions' variance can be explained by the contents of the image itself - or essentially how much our model is actually using those pixels to generate a conclusion."
      ],
      "metadata": {
        "id": "3y-9xAYuTbWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('METRICS ON ENTIRE DATASET:')\n",
        "rmse = np.sqrt(cnn.evaluate(test_images, verbose=0))\n",
        "print(\"Test RMSE:\\t{:.5f}\".format(rmse))\n",
        "r2 = r2_score(true_weights,predicted_weights)\n",
        "print(\"Test R^2 Score:\\t{:.5f}\".format(r2))\n",
        "print('--------------------------')\n",
        "print('METRICS ON SELECTED COWS:')\n",
        "cv_rmse = np.sqrt(cnn.evaluate(selected_images, verbose=0))\n",
        "print(\"CV RMSE:\\t{:.5f}\".format(rmse))\n",
        "cv_r2 = r2_score(selected_true_weights,selected_predicted_weights)\n",
        "print(\"CV R^2 Score:\\t{:.5f}\".format(r2))\n",
        "print('--------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w75hSOQRiGg",
        "outputId": "52688346-3337-4197-abb9-cd7768b4dada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "METRICS ON ENTIRE DATASET:\n",
            "Test RMSE:\t260.65550\n",
            "Test R^2 Score:\t0.17656\n",
            "--------------------------\n",
            "METRICS ON SELECTED COWS:\n",
            "CV RMSE:\t260.65550\n",
            "CV R^2 Score:\t0.17656\n",
            "--------------------------\n"
          ]
        }
      ]
    }
  ]
}
